{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *corpkit*: a Python-based toolkit for working with linguistic corpora\n",
    "\n",
    "[![Join the chat at https://gitter.im/interrogator/corpkit](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/interrogator/corpkit?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![DOI](https://zenodo.org/badge/14568/interrogator/corpkit.svg)](https://zenodo.org/badge/latestdoi/14568/interrogator/corpkit) [![PyPI](https://img.shields.io/pypi/v/corpkit.svg)](https://pypi.python.org/pypi/corpkit)\n",
    "\n",
    "### Daniel McDonald ([@interro_gator](https://twitter.com/interro_gator))\n",
    "\n",
    "> Because I kept building new tools and functions for my linguistic research, I decided to put them together into `corpkit`, a simple toolkit for working with parsed and structured linguistic corpora.\n",
    "> \n",
    "> **Recently, I've been working on a GUI version of the tool. This resides as pure Python in `corpkit/corpkit-gui.py`. There is also a zipped up version of an OSX executable in the [`corpkit-app` submodule](https://www.github.com/interrogator/corpkit-app). Documentation for the GUI is [here](http://interrogator.github.io/corpkit/). From here on out, though, this readme concerns the command line interface only.**\n",
    "\n",
    "<!-- MarkdownTOC -->\n",
    "\n",
    "- [What's in here?](#whats-in-here)\n",
    "  - [Key features](#key-features)\n",
    "    - [`make_corpus()`](#make_corpus)\n",
    "    - [`interrogator()`](#interrogator)\n",
    "    - [`editor()`](#editor)\n",
    "    - [`plotter()`](#plotter)\n",
    "    - [Other stuff](#other-stuff)\n",
    "- [Installation](#installation)\n",
    "  - [By downloading the repository](#by-downloading-the-repository)\n",
    "  - [By cloning the repository](#by-cloning-the-repository)\n",
    "  - [Via `pip`](#via-pip)\n",
    "- [Unpacking the orientation data](#unpacking-the-orientation-data)\n",
    "- [Quickstart](#quickstart)\n",
    "- [More detailed examples](#more-detailed-examples)\n",
    "  - [Building corpora](#building-corpora)\n",
    "    - [Speaker IDs](#speaker-ids)\n",
    "  - [Concordancing](#concordancing)\n",
    "  - [Systemic functional stuff](#systemic-functional-stuff)\n",
    "  - [Keywording](#keywording)\n",
    "    - [Plotting keywords](#plotting-keywords)\n",
    "    - [Traditional reference corpora](#traditional-reference-corpora)\n",
    "  - [Parallel processing](#parallel-processing)\n",
    "  - [More complex queries and plots](#more-complex-queries-and-plots)\n",
    "  - [Visualisation options](#visualisation-options)\n",
    "- [More information](#more-information)\n",
    "- [Cite](#cite)\n",
    "\n",
    "<!-- /MarkdownTOC -->\n",
    "\n",
    "<a name=\"whats-in-here\"></a>\n",
    "## What's in here?\n",
    "\n",
    "Essentially, the module contains functions for building and interrogating corpora, then manipulating or visualising the results. The most important of them are:\n",
    "\n",
    "| **Function name** | Purpose                            | \n",
    "| ----------------- | ---------------------------------- | \n",
    "| `make_corpus()`   | Use CoreNLP/NLTK to make to make parsed, speaker-segmented corpora from plain text files      |\n",
    "| `interrogator()`  | interrogate parse trees, dependencies, or find keywords or ngrams in parsed corpora | \n",
    "| `editor()`        | edit `interrogator()` or `conc()` results, calculate keywords, sort using linear regression, etc. |\n",
    "| `plotter()`       | visualise results via *matplotlib* | \n",
    "| `conc()`          | Lexicogrammatical concordancing via constituency/dependency annotations | \n",
    "\n",
    "There are also quite a few helper functions for making regular expressions, saving and loading data, making new projects, and so on.\n",
    "\n",
    "Also included are some lists of words and dependency roles, which can be used to match functional linguistic categories. These are explained in more detail [here](#systemic-functional-stuff).\n",
    "\n",
    "While most of the tools are designed to work with corpora that are parsed (by [Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml)) and structured (in a series of directories representing different points in time, speaker IDs, chapters of a book, etc.), the tools can also be used on text that is unparsed and/or unstructured. That said, you won't be able to do nearly as much cool stuff.\n",
    "\n",
    "The idea is to run the tools from a [Jupyter Notebook](http://jupyter.org), but you could also operate the toolkit from the command line if you wanted to have less fun.\n",
    "\n",
    "The most comprehensive use of `corpkit` to date has been for an investigation of the word *risk* in *The New York Times*, 1963&ndash;2014. The repository for that project is [here](https://www.github.com/interrogator/risk); the Notebook demonstrating the use of `corpkit` can be viewed via either [GitHub](https://github.com/interrogator/risk/blob/master/risk.ipynb) or [NBviewer](http://nbviewer.ipython.org/github/interrogator/risk/blob/master/risk.ipynb).\n",
    "\n",
    "<a name=\"key-features\"></a>\n",
    "### Key features\n",
    "\n",
    "<a name=\"make_corpus\"></a>\n",
    "#### `make_corpus()` \n",
    "\n",
    "* A simple Python wrapper around CoreNLP\n",
    "* Creates a parsed version of a structured or unstructured plaintext corpus\n",
    "* Optionally detects speaker IDs and adds them to CoreNLP XML\n",
    "\n",
    "\n",
    "<a name=\"interrogator\"></a>\n",
    "#### `interrogator()`\n",
    "\n",
    "* Use [Tregex](http://nlp.stanford.edu/~manning/courses/ling289/Tregex.html) or regular expressions to search parse trees or plain text for complex lexicogrammatical phenomena\n",
    "* Search Stanford dependencies (whichever variety you like) for information about the role, governor, dependent or index of a token matching a regular expression\n",
    "* Return words or phrases, POS/group/phrase tags, raw counts, or all three.\n",
    "* Return lemmatised or unlemmatised results (using WordNet for constituency trees, and CoreNLP's lemmatisation for dependencies). Add words to `dictionaries/word_transforms.py` manually if need be\n",
    "* Look for keywords in each subcorpus (using code from [*Spindle*](https://github.com/sgrau/spindle-code)), and chart their keyness\n",
    "* Look for ngrams in each subcorpus, and chart their frequency\n",
    "* Two-way UK-US spelling conversion (superior as the former may be), and the ability to add words manually\n",
    "* Output Pandas DataFrames that can be easily edited and visualised\n",
    "* Use parallel processing to search for a number of patterns, or search for the same pattern in multiple corpora\n",
    "\n",
    "<a name=\"editor\"></a>\n",
    "#### `editor()`\n",
    "\n",
    "* Remove, keep or merge interrogation results or subcorpora using indices, words or regular expressions (see below)\n",
    "* Sort results by name or total frequency\n",
    "* Use linear regression to figure out the trajectories of results, and sort by the most increasing, decreasing or static values\n",
    "* Show the *p*-value for linear regression slopes, or exclude results above *p*\n",
    "* Work with absolute frequency, or determine ratios/percentage of another list: \n",
    "    * determine the total number of verbs, or total number of verbs that are *be*\n",
    "    * determine the percentage of verbs that are *be*\n",
    "    * determine the percentage of *be* verbs that are *was*\n",
    "    * determine the ratio of *was/were* ...\n",
    "    * etc.\n",
    "* Plot more advanced kinds of relative frequency: for example, find all proper nouns that are subjects of clauses, and plot each word as a percentage of all instances of that word in the corpus (see below)\n",
    "\n",
    "<a name=\"plotter\"></a>\n",
    "#### `plotter()` \n",
    "\n",
    "* Plot using *Pandas*/*Matplotlib*\n",
    "* Interactive plots (hover-over text, interactive legends) using *mpld3* (examples in the [*Risk Semantics* notebook](https://github.com/interrogator/risk/blob/master/risk.ipynb))\n",
    "* Plot anything you like: words, tags, counts for grammatical features ...\n",
    "* Create line charts, bar charts, pie charts, etc. with the `kind` argument\n",
    "* Use `subplots = True` to produce individual charts for each result\n",
    "* Customisable figure titles, axes labels, legends, image size, colormaps, etc.\n",
    "* Use `TeX` if you have it\n",
    "* Use log scales if you really want\n",
    "* Use a number of chart styles, such as `ggplot` or `fivethirtyeight`\n",
    "* Save images to file, as `.pdf` or `.png`\n",
    "\n",
    "<a name=\"other-stuff\"></a>\n",
    "#### Other stuff\n",
    "\n",
    "* View top results as a table via `Pandas`\n",
    "* Standalone tools for quickly and easily generating lists of keywords, ngrams, collocates and concordances\n",
    "* Concordance using Tregex (i.e. concordance all nominal groups containing *gross* as an adjective with `NP < (JJ < /gross/)`)\n",
    "* Randomise concordance results, determine window size, output to CSV, etc.\n",
    "* Quickly save interrogations and figures to file, and reload results in new sessions with `save_result()` and `load_result()`\n",
    "* Build dictionaries from corpora, subcorpora or concordance lines, which can then be used as reference corpora for keyword generation\n",
    "* Start a new blank project with `new_project()`\n",
    "\n",
    "One of the main reasons for these tools was to make it quicker and easier to explore corpora in complex ways, using output from one tool as input for the next.\n",
    "\n",
    "* n-gramming and keywording can be done via `interrogator()`\n",
    "* keywording can easily be done on your concordance lines\n",
    "* Use loops to concordance the top results from an interrogation, or check their keyness\n",
    "* use `editor()` to edit concordance line output as well as interrogations\n",
    "* build a dictionary from a corpus, subcorpus, or from concordance lines, and use it as a reference corpus for keywording\n",
    "* Restrict keyword analysis to particular parts of lexis/grammar (i.e. NP heads), removing the need for stopword lists, and making topic summarisation easier\n",
    "* Use `interrogator()` output or subset of output as target or reference corpus\n",
    "* and so on ...\n",
    "\n",
    "Included here is `orientation/orientation.ipynb`, which is a Jupyter Notebook version of this readme. In `orientation/data` is a sample corpus of paragraph from *The New York Times* containing the word *risk*. Due to size restrictions, This data only includes parse trees (no dependencies), and isn't included in the pip package. With the notebook and the data, you can easily run all the code in this document yourself.\n",
    "\n",
    "<a name=\"installation\"></a>\n",
    "## Installation\n",
    "\n",
    "You can get `corpkit` running by downloading or cloning this repository, or via `pip`.\n",
    "\n",
    "<a name=\"by-downloading-the-repository\"></a>\n",
    "### By downloading the repository\n",
    "\n",
    "Hit 'Download ZIP' and unzip the file. Then `cd` into the newly created directory and install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "shell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "cd corpkit-master\n",
    "# might need sudo:\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"by-cloning-the-repository\"></a>\n",
    "### By cloning the repository\n",
    "\n",
    "Clone the repo, `cd` into it and run the setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "shell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/interrogator/corpkit.git\n",
    "cd corpkit\n",
    "# might need sudo:\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"via-pip\"></a>\n",
    "### Via `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "shell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# might need sudo:\n",
    "pip install corpkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*corpkit* should install all the necessary dependencies, including *pandas*, *NLTK*, *matplotlib*, etc, as well as some NLTK data files. \n",
    "\n",
    "If you get an NLTK error, you might need to manually download the tokeniser and lemmatiser data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "# change 'punkt' to 'all' to get everything\n",
    ">>> nltk.download('punkt')\n",
    ">>> nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"unpacking-the-orientation-data\"></a>\n",
    "## Unpacking the orientation data\n",
    "\n",
    "If you installed by downloading or cloning this repository, you'll have the orientation project installed. To use it, `cd` into the orientation project and unzip the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "shell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "cd orientation\n",
    "# unzip data to data folder\n",
    "gzip -dc data/nyt.tar.gz | tar -xf - -C data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"quickstart\"></a>\n",
    "## Quickstart\n",
    "\n",
    "The best way to use `corpkit` is by opening `orientation/orientation.ipynb` with Jupyter, and executing the relevant cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "shell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "ipython notebook orientation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, just use *(I)Python*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import corpkit\n",
    ">>> from corpkit import interroplot\n",
    "\n",
    "# set corpus path\n",
    ">>> corpus = 'data/nyt/years'\n",
    "\n",
    "# search nyt for modal auxiliaries:\n",
    ">>> interroplot(corpus, r'MD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \n",
    "\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/md2.png\" />\n",
    "<br><br>\n",
    "\n",
    "<a name=\"more-detailed-examples\"></a>\n",
    "## More detailed examples\n",
    "\n",
    "`interroplot()` is just a demo function that does three things in order:\n",
    "\n",
    "1. uses `interrogator()` to search corpus for a (Regex or Tregex) query\n",
    "2. uses `editor()` to calculate the relative frequencies of each result\n",
    "3. uses `plotter()` to show the top seven results\n",
    " \n",
    "Here's an example of the three functions at work on the NYT corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from corpkit import interrogator, editor, plotter\n",
    "# make tregex query: head of NP in PP containing 'of'\n",
    "# in NP headed by risk word:\n",
    ">>> q = r'/NN.?/ >># (NP > (PP <<# /(?i)of/ > (NP <<# (/NN.?/ < /(?i).?\\brisk.?/))))'\n",
    "\n",
    "# count terminals/leaves of trees only, and do lemmatisation:\n",
    ">>> risk_of = interrogator(corpus, 'words', q, lemmatise = True)\n",
    "\n",
    "# use editor to turn absolute into relative frequencies\n",
    ">>> to_plot = editor(risk_of.results, '%', risk_of.totals)\n",
    "\n",
    "# plot the results\n",
    ">>> plotter('Risk of (noun)', to_plot.results, y_label = 'Percentage of all results',\n",
    "...    style = 'fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \n",
    "\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/risk-of-noun.png\" />\n",
    "<br><br>\n",
    "\n",
    "<a name=\"building-corpora\"></a>\n",
    "### Building corpora\n",
    "\n",
    "*corpkit* contains a modest function for created parsed and/or tokenised corpora. The main thing you need is **a folder, containing either text files, or subfolders that contain text files**. If you want to parse the corpus, you'll also need to have downloaded and unzipped [Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml). If you're tokenising, you'll need to make sure you have NLTK's tokeniser data. You can then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> parsed = make_corpus(unparsed, parse = True, tokenise = True,\n",
    "...    corenlppath = 'Downloads/corenlp', nltk_data_path = 'Downloads/nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which creates the parsed corpora, and returns their paths. You can also optionally pass in a string of annotators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = 'tokenize,ssplit,pos'\n",
    "parsed = make_corpus(unparsed, operations = ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"speaker-ids\"></a>\n",
    "#### Speaker IDs\n",
    "\n",
    "Something novel about *corpkit* is that it can work with corpora containing speaker IDs (scripts, transcripts, logs, etc.), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOHN: Why did they change the signs above all the bins?\n",
    "SPEAKER23: I know why. But I'm not telling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> parsed = make_corpus(path, speaker_segmentation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will:\n",
    "\n",
    "1. Detect any IDs in any file\n",
    "2. Create a duplicate version of the corpus with IDs removed\n",
    "3. Parse this 'cleaned' corpus\n",
    "4. Add an XML tag to each sentence with the name of the speaker\n",
    "5. Return paths to the cleaned and parsed corpora\n",
    "\n",
    "When interrogating or concordancing, you can then pass in a keyword argument to restrict searches to one or more speakers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> s = ['BRISCOE', 'LOGAN']\n",
    ">>> npheads = interrogator(parsed, 'words', r'/NN.?/ >># NP', just_speakers = s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it possible to not only investigate individual speakers, but to form an understanding of the overall tenor/tone of the text as well: *Who does most of the talking? Who is asking the questions? Who issues commands?*\n",
    "\n",
    "<a name=\"concordancing\"></a>\n",
    "### Concordancing\n",
    "\n",
    "Unlike most concordancers, which are based on plaintext corpora, *corpkit* can concordance via Tregex queries or dependency tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from corpkit import conc\n",
    "\n",
    ">>> subcorpus = 'data/nyt/years/2005'\n",
    ">>> query = r'/JJ.?/ > (NP <<# (/NN.?/ < /\\brisk/))'\n",
    ">>> lines = conc(subcorpus, 'tregex', query, window = 50, n = 10, random = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output (a `Pandas DataFrame`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0    hedge funds or high-risk stocks obviously poses a         greater   risk to the pension program than a portfolio of   \n",
    "1           contaminated water pose serious health and   environmental   risks                                             \n",
    "2   a cash break-even pace '' was intended to minimize       financial   risk to the parent company                        \n",
    "3                                                Other           major   risks identified within days of the attack        \n",
    "4                           One seeks out stocks ; the           other   monitors risks                                    \n",
    "5        men and women in Colorado Springs who were at            high   risk for H.I.V. infection , because of            \n",
    "6   by the marketing consultant Seth Godin , to taking      calculated   risks , in the opinion of two longtime business   \n",
    "7        to happen '' in premises '' where there was a            high   risk of fire                                      \n",
    "8       As this was match points , some of them took a          slight   risk at the second trick by finessing the heart   \n",
    "9     said that the agency 's continuing review of how         Guidant   treated patient risks posed by devices like the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching dependencies, you have the added ability to restrict to a particular role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> query = r'\\bm(a|e)n\\b'\n",
    ">>> lines = conc(subcorpus, 'deps', query, dep_function = 'nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can search tokenised corpora or plaintext corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> lines = conc(subcorpus, 'plaintext', query)\n",
    ">>> lines = conc(subcorpus, 'tokens', query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `query` can be a regular expression or list of words to match.\n",
    "\n",
    "If you really wanted, you can then go on to use `conc()` output as a dictionary, or extract keywords and ngrams from it, or keep or remove certain results with `editor()`. If you want to [give the GUI a try](http://interrogator.github.io/corpkit/), you can colour-code and create thematic categories for concordance lines as well.\n",
    "\n",
    "<a name=\"systemic-functional-stuff\"></a>\n",
    "### Systemic functional stuff\n",
    "\n",
    "Because I mostly use systemic functional grammar, there is also a simple tool for distinguishing between process types (relational, mental, verbal) when interrogating a corpus. If you add words to the lists in `dictionaries/process_types.py`, corpkit will get their inflections automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from corpkit import quickview\n",
    ">>> from dictionaries.process_types import processes\n",
    "\n",
    "# use verbal process regex as the query\n",
    "# deprole finds the dependent of verbal processes, and its functional role\n",
    "# keep only results matching function_filter regex\n",
    ">>> sayers = interrogator(corpus, 'd', processes.verbal, \n",
    "...    function_filter = r'^nsubj$', lemmatise = True)\n",
    "\n",
    "# have a look at the top results\n",
    ">>> quickview(sayers, n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  0: he (n=24530)\n",
    "  1: she (n=5558)\n",
    "  2: they (n=5510)\n",
    "  3: official (n=4348)\n",
    "  4: it (n=3752)\n",
    "  5: who (n=2940)\n",
    "  6: that (n=2665)\n",
    "  7: i (n=2062)\n",
    "  8: expert (n=2057)\n",
    "  9: analyst (n=1369)\n",
    " 10: we (n=1214)\n",
    " 11: report (n=1103)\n",
    " 12: company (n=1070)\n",
    " 13: which (n=1043)\n",
    " 14: you (n=987)\n",
    " 15: researcher (n=987)\n",
    " 16: study (n=901)\n",
    " 17: critic (n=826)\n",
    " 18: person (n=802)\n",
    " 19: agency (n=798)\n",
    " 20: doctor (n=770)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try removing the pronouns using `editor()`. The quickest way is to use the editable wordlists stored in `dictionaries/wordlists`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from corpkit import editor\n",
    ">>> from dictionaries.wordlists import wordlists\n",
    ">>> prps = wordlists.pronouns\n",
    "\n",
    "# alternative approaches:\n",
    "# >>> prps = [0, 1, 2, 4, 5, 6, 7, 10, 13, 14, 24]\n",
    "# >>> prps = ['he', 'she', 'you']\n",
    "# >>> prps = as_regex(wl.pronouns, boundaries = 'line')\n",
    "\n",
    "# give editor() indices, words, wordlists or regexes to keep remove or merge\n",
    ">>> sayers_no_prp = editor(sayers.results, skip_entries = prps,\n",
    "...    skip_subcorpora = [1963])\n",
    ">>> quickview(sayers_no_prp, n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  0: official (n=4342)\n",
    "  1: expert (n=2055)\n",
    "  2: analyst (n=1369)\n",
    "  3: report (n=1098)\n",
    "  4: company (n=1066)\n",
    "  5: researcher (n=987)\n",
    "  6: study (n=900)\n",
    "  7: critic (n=825)\n",
    "  8: person (n=801)\n",
    "  9: agency (n=796)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now, let's sort the entries by trajectory, and then plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort with editor()\n",
    ">>> sayers_no_prp = editor(sayers_no_prp.results, '%', sayers.totals, sort_by = 'increase')\n",
    "\n",
    "# make an area chart with custom y label\n",
    ">>> plotter('Sayers, increasing', sayers_no_prp.results, kind = 'area', \n",
    "...    y_label = 'Percentage of all sayers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/sayers-increasing.png\" />\n",
    "<br><br>\n",
    "\n",
    "We can also merge subcorpora. Let's look for changes in gendered pronouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> merges = {'1960s': r'^196', \n",
    "...           '1980s': r'^198', \n",
    "...           '1990s': r'^199', \n",
    "...           '2000s': r'^200',\n",
    "...           '2010s': r'^201'}\n",
    "\n",
    ">>> sayers = editor(sayers.results, merge_subcorpora = merges)\n",
    "\n",
    "# now, get relative frequencies for he and she\n",
    ">>> genders = editor(sayers.results, '%', sayers.totals, just_entries = ['he', 'she'])\n",
    "\n",
    "# and plot it as a series of pie charts, showing totals on the slices:\n",
    ">>> plotter('Pronominal sayers in the NYT', genders.results.T, kind = 'pie',\n",
    "...    subplots = True, figsize = (15, 2.75), show_totals = 'plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/ann_he_she.png\" />\n",
    "<br><br>\n",
    "\n",
    "Woohoo, a decreasing gender divide! \n",
    "\n",
    "<a name=\"keywording\"></a>\n",
    "### Keywording\n",
    "\n",
    "As I see it, there are two main problems with keywording, as typically performed in corpus linguistics. First is the reliance on 'balanced'/'general' reference corpora, which are obviously a fiction. Second is the idea of stopwords. Essentially, when most people calculate keywords, they use stopword lists to automatically filter out words that they think will not be of interest to them. These words are generally closed class words, like determiners, prepositions, or pronouns. This is not a good way to go about things: the relative frequencies of *I*, *you* and *one* can tell us a lot about the kinds of language in a corpus. More seriously, stopwords mean adding subjective judgements about what is interesting language into a process that is useful precisely because it is not subjective or biased.\n",
    "\n",
    "So, what to do? Well, first, don't use 'general reference corpora' unless you really really have to. With `corpkit`, you can use your entire corpus as the reference corpus, and look for keywords in subcorpora. Second, rather than using lists of stopwords, simply do not send all words in the corpus to the keyworder for calculation. Instead, try looking for key *predicators* (rightmost verbs in the VP), or key *participants* (heads of arguments of these VPs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just heads of participants (no pronouns, though!)\n",
    ">>> part = r'/(NN|JJ).?/ >># (/(NP|ADJP)/ $ VP | > VP)'\n",
    ">>> p = interrogator(corpus, 'words', part, lemmatise = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `editor()` to calculate keywords, there are a few default parameters that can be easily changed:\n",
    "\n",
    "| Keyword argument | Function | Default setting | Type\n",
    "|---|---|---|---|\n",
    "| `threshold`  | Remove words occurring fewer than `n` times in reference corpus  | `False` | `'high/medium/low'`/ `True/False` / `int`\n",
    "| `calc_all`  | Calculate keyness for words in both reference and target corpus, rather than just target corpus  | `True` | `True/False`\n",
    "| `selfdrop`  | Attempt to remove target data from reference data when calculating keyness  | `True`  | `True/False`\n",
    "\n",
    "Let's have a look at how these options change the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'self' as reference corpus uses p.results\n",
    ">>> options = {'selfdrop': True, \n",
    "...            'selfdrop': False, \n",
    "...            'calc_all': False, \n",
    "...            'threshold': False}\n",
    "\n",
    ">>> for k, v in options.items():\n",
    "...    k = editor(p.results, 'keywords', 'self', k = v)\n",
    "...    print k.results.ix['2011'].order(ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "| #1: default       | |   #2: no `selfdrop` | |  #3: no `calc_all`    |  |  #4: no `threshold` | |\n",
    "|---|---:|---|---:|---|---:|---|---:|\n",
    "| risk        | 1941.47  |  risk        |  1909.79  |  risk        | 1941.47   |  bank       |   668.19 |\n",
    "| bank        | 1365.70  |  bank        |  1247.51  |  bank        | 1365.70   |  crisis     |   242.05 |\n",
    "| crisis      |  431.36  |  crisis      |   388.01  |  crisis      |  431.36   |  obama      |   172.41 |\n",
    "| investor    |  410.06  |  investor    |   387.08  |  investor    |  410.06   |  demiraj    |   161.90 |\n",
    "| rule        |  316.77  |  rule        |   293.33  |  rule        |  316.77   |  regulator  |   144.91 |\n",
    "|             |   ...    |              |    ...    |              |   ...     |             |    ...   |\n",
    "| clinton     |  -37.80  |  tactic      |   -35.09  |  hussein     |  -25.42   |  clinton    |   -87.33 |\n",
    "| vioxx       |  -38.00  |  vioxx       |   -35.29  |  clinton     |  -37.80   |  today      |   -89.49 |\n",
    "| greenspan   |  -54.35  |  greenspan   |   -51.38  |  vioxx       |  -38.00   |  risky      |  -125.76 |\n",
    "| bush        | -153.06  |  bush        |  -143.02  |  bush        | -153.06   |  bush       |  -253.95 |\n",
    "| yesterday   | -162.30  |  yesterday   |  -151.71  |  yesterday   | -162.30   |  yesterday  |  -268.29 |\n",
    "\n",
    "As you can see, slight variations on keywording give different impressions of the same corpus!\n",
    "\n",
    "A key strength of `corpkit`'s approach to keywording is that you can generate new keyword lists without re-interrogating the corpus. Let's use Pandas syntax to look for keywords in the past few years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> yrs = ['2011', '2012', '2013', '2014']\n",
    ">>> keys = editor(p.results.ix[yrs].sum(), 'keywords', p.results.drop(yrs),\n",
    "...    threshold = False)\n",
    ">>> print keys.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank          1795.24\n",
    "obama          722.36\n",
    "romney         560.67\n",
    "jpmorgan       527.57\n",
    "rule           413.94\n",
    "dimon          389.86\n",
    "draghi         349.80\n",
    "regulator      317.82\n",
    "italy          282.00\n",
    "crisis         243.43\n",
    "putin          209.51\n",
    "greece         208.80\n",
    "snowden        208.35\n",
    "mf             192.78\n",
    "adoboli        161.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or track the keyness of a set of words over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> terror = ['terror', 'terrorism', 'terrorist']\n",
    ">>> terr = editor(p.results, 'k', 'self', merge_entries = terror, newname = 'terror')\n",
    ">>> print terr.results.terror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1963    -2.51\n",
    "1987    -3.67\n",
    "1988   -16.09\n",
    "1989    -6.24\n",
    "1990   -16.24\n",
    "...       ...\n",
    "Name: terror, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"plotting-keywords\"></a>\n",
    "#### Plotting keywords\n",
    "\n",
    "Naturally, we can use `plotter()` for our keywords too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> plotter('Terror* as Participant in the \\emph{NYT}', pols.results.terror, \n",
    "...    kind = 'area', stacked = False, y_label = 'L/L Keyness')\n",
    ">>> politicians = ['bush', 'obama', 'gore', 'clinton', 'mccain', \n",
    "...                'romney', 'dole', 'reagan', 'gorbachev']\n",
    ">>> plotter('Keyness of politicians in the \\emph{NYT}', k.results[politicans], \n",
    "...    num_to_plot = 'all', y_label = 'L/L Keyness', kind = 'area', legend_pos = 'center left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/terror-as-participant-in-the-emphnyt.png\" />\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/keyness-of-politicians-in-the-emphnyt.png\" />\n",
    "<br><br>\n",
    "\n",
    "<a name=\"traditional-reference-corpora\"></a>\n",
    "#### Traditional reference corpora\n",
    "\n",
    "If you still want to use a standard reference corpus, you can do that (and a dictionary version of the BNC is included). For the reference corpus, `editor()` recognises `dicts`, `DataFrames`, `Series`, files containing `dicts`, or paths to plain text files or trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary list of common/boring words\n",
    ">>> from dictionaries.stopwords import stopwords\n",
    ">>> print editor(p.results.ix['2013'], 'k', 'bnc.p', \n",
    "...    skip_entries = stopwords).results\n",
    ">>> print editor(p.results.ix['2013'], 'k', 'bnc.p', calc_all = False).results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output (not so useful):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1                                #2\n",
    "bank           5568.25            bank      5568.25\n",
    "person         5423.24            person    5423.24\n",
    "company        3839.14            company   3839.14\n",
    "way            3537.16            way       3537.16\n",
    "state          2873.94            state     2873.94\n",
    "                ...                           ...  \n",
    "three          -691.25            ten       -199.36\n",
    "people         -829.97            bit       -205.97\n",
    "going          -877.83            sort      -254.71\n",
    "erm           -2429.29            thought   -255.72\n",
    "yeah          -3179.90            will      -679.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the record, you could also use `interrogator()` or `keywords()` to calculate keywords, though these options may offer less flexibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from corpkit import keywords\n",
    ">>> keys = keywords(p.results.ix['2002'], reference_corpus = p.results])\n",
    ">>> keys = interrogator(corpus, 'keywords', 'any', reference_corpus = 'self')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"parallel-processing\"></a>\n",
    "### Parallel processing\n",
    "\n",
    "`interrogator()` can also parallel-process multiple queries or corpora. Parallel processing will be automatically enabled if you pass in either:\n",
    "\n",
    "1. a `list` of paths as `path` (i.e. `['path/to/corpus1', 'path/to/corpus2']`)\n",
    "2. a `dict` as `query` (i.e. `{'Noun phrases': r'NP', 'Verb phrases': r'VP'}`)\n",
    "\n",
    "Let's look at different risk processes (e.g. *risk*, *take risk*, *run risk*, *pose risk*, *put at risk*) using constituency parses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> query = {'risk': r'VP <<# (/VB.?/ < /(?i).?\\brisk.?\\b/)', \n",
    "...    'take risk': r'VP <<# (/VB.?/ < /(?i)\\b(take|takes|taking|took|taken)+\\b/) < (NP <<# /(?i).?\\brisk.?\\b/)', \n",
    "...    'run risk': r'VP <<# (/VB.?/ < /(?i)\\b(run|runs|running|ran)+\\b/) < (NP <<# /(?i).?\\brisk.?\\b/)', \n",
    "...    'put at risk': r'VP <<# /(?i)(put|puts|putting)\\b/ << (PP <<# /(?i)at/ < (NP <<# /(?i).?\\brisk.?/))', \n",
    "...    'pose risk': r'VP <<# (/VB.?/ < /(?i)\\b(pose|poses|posed|posing)+\\b/) < (NP <<# /(?i).?\\brisk.?\\b/)'}\n",
    "\n",
    ">>> processes = interrogator(corpus, 'count', query)\n",
    ">>> proc_rel = editor(processes.results, '%', processes.totals)\n",
    ">>> plotter('Risk processes', proc_rel.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/risk_processes-2.png\" />\n",
    "<br><br>\n",
    "\n",
    "<a name=\"more-complex-queries-and-plots\"></a>\n",
    "### More complex queries and plots\n",
    "\n",
    "Next, let's find out what kinds of noun lemmas are subjects of any of these risk processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a query to find heads of nps that are subjects of risk processes\n",
    ">>> query = r'/^NN(S|)$/ !< /(?i).?\\brisk.?/ >># (@NP $ (VP <+(VP) (VP ( <<# (/VB.?/ < /(?i).?\\brisk.?/) ' \\\n",
    "...    r'| <<# (/VB.?/ < /(?i)\\b(take|taking|takes|taken|took|run|running|runs|ran|put|putting|puts)/) < ' \\\n",
    "...    r'(NP <<# (/NN.?/ < /(?i).?\\brisk.?/))))))'\n",
    ">>> noun_riskers = interrogator(c, 'words', query, lemmatise = True)\n",
    " \n",
    ">>> quickview(noun_riskers, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  0: person (n=195)\n",
    "  1: company (n=139)\n",
    "  2: bank (n=80)\n",
    "  3: investor (n=66)\n",
    "  4: government (n=63)\n",
    "  5: man (n=51)\n",
    "  6: leader (n=48)\n",
    "  7: woman (n=43)\n",
    "  8: official (n=40)\n",
    "  9: player (n=39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `editor()` to make some thematic categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everyday people\n",
    ">>> p = ['person', 'man', 'woman', 'child', 'consumer', 'baby', 'student', 'patient']\n",
    "\n",
    ">>> them_cat = editor(noun_riskers.results, merge_entries = p, newname = 'Everyday people')\n",
    "\n",
    "# get business, gov, institutions\n",
    ">>> i = ['company', 'bank', 'investor', 'government', 'leader', 'president', 'officer', \n",
    "...      'politician', 'institution', 'agency', 'candidate', 'firm']\n",
    "\n",
    ">>> them_cat = editor(them_cat.results, '%', noun_riskers.totals, merge_entries = i, \n",
    "...    newname = 'Institutions', sort_by = 'total', skip_subcorpora = 1963,\n",
    "...    just_entries = ['Everyday people', 'Institutions'])\n",
    "\n",
    "# plot result\n",
    ">>> plotter('Types of riskers', them_cat.results, y_label = 'Percentage of all riskers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/types-of-riskers.png\" />\n",
    "<br><br>\n",
    "\n",
    "Let's also find out what percentage of the time some nouns appear as riskers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find any head of an np not containing risk\n",
    ">>> query = r'/NN.?/ >># NP !< /(?i).?\\brisk.?/'\n",
    ">>> noun_lemmata = interrogator(corpus, 'words', query, lemmatise = True)\n",
    "\n",
    "# get some key terms\n",
    ">>> people = ['man', 'woman', 'child', 'baby', 'politician', \n",
    "...           'senator', 'obama', 'clinton', 'bush']\n",
    ">>> selected = editor(noun_riskers.results, '%', noun_lemmata.results, \n",
    "...    just_entries = people, just_totals = True, threshold = 0, sort_by = 'total')\n",
    "\n",
    "# make a bar chart:\n",
    ">>> plotter('Risk and power', selected.results, num_to_plot = 'all', kind = 'bar', \n",
    "...    x_label = 'Word', y_label = 'Risker percentage', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "<img style=\"float:left\" src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/risk-and-power-2.png\" />\n",
    "<br><br>\n",
    "\n",
    "<a name=\"visualisation-options\"></a>\n",
    "### Visualisation options\n",
    "\n",
    "With a bit of creativity, you can do some pretty awesome data-viz, thanks to *Pandas* and *Matplotlib*. The following plots require only one interrogation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> modals = interrogator(annual_trees, 'words', 'MD < __')\n",
    "# simple stuff: make relative frequencies for individual or total results\n",
    ">>> rel_modals = editor(modals.results, '%', modals.totals)\n",
    "\n",
    "# trickier: make an 'others' result from low-total entries\n",
    ">>> low_indices = range(7, modals.results.shape[1])\n",
    ">>> each_md = editor(modals.results, '%', modals.totals, merge_entries = low_indices, \n",
    "...    newname = 'other', sort_by = 'total', just_totals = True, keep_top = 7)\n",
    "\n",
    "# complex stuff: merge results\n",
    ">>> entries_to_merge = [r'(^w|\\'ll|\\'d)', r'^c', r'^m', r'^sh']\n",
    ">>> modals = editor(modals.results, merge_entries = entries_to_merge)\n",
    "    \n",
    "# complex stuff: merge subcorpora\n",
    ">>> merges = {'1960s': r'^196', \n",
    "...           '1980s': r'^198', \n",
    "...           '1990s': r'^199', \n",
    "...           '2000s': r'^200',\n",
    "...           '2010s': r'^201'}\n",
    "\n",
    ">>> modals = editor(sayers.results, merge_subcorpora = merges)\n",
    "    \n",
    "# make relative, sort, remove what we don't want\n",
    ">>> modals = editor(modals.results, '%', modals.totals, keep_stats = False,\n",
    "...    just_subcorpora = merges.keys(), sort_by = 'total', keep_top = 4)\n",
    "\n",
    "# show results\n",
    ">>> print rel_modals.results, each_md.results, modals.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          would       will        can      could  ...        need     shall      dare  shalt\n",
    "1963  22.326833  23.537323  17.955615   6.590451  ...    0.000000  0.537996  0.000000      0\n",
    "1987  24.750614  18.505132  15.512505  11.117537  ...    0.072286  0.260228  0.014457      0\n",
    "1988  23.138986  19.257117  16.182067  11.219364  ...    0.091338  0.060892  0.000000      0\n",
    "...         ...        ...        ...        ...  ...         ...       ...       ...    ...\n",
    "2012  23.097345  16.283186  15.132743  15.353982  ...    0.029499  0.029499  0.000000      0\n",
    "2013  22.136269  17.286522  16.349301  15.620351  ...    0.029753  0.029753  0.000000      0\n",
    "2014  21.618357  17.101449  16.908213  14.347826  ...    0.024155  0.000000  0.000000      0\n",
    "[29 rows x 17 columns] \n",
    "\n",
    "would     23.235853\n",
    "will      17.484034\n",
    "can       15.844070\n",
    "could     13.243449\n",
    "may        9.581255\n",
    "should     7.292294\n",
    "other      7.290155\n",
    "Name: Combined total, dtype: float64 \n",
    "\n",
    "       would/will/'ll...  can/could/ca  may/might/must  should/shall/shalt\n",
    "1960s          47.276395     25.016812       19.569603            7.800941\n",
    "1980s          44.756285     28.050776       19.224476            7.566817\n",
    "1990s          44.481957     29.142571       19.140310            6.892708\n",
    "2000s          42.386571     30.710739       19.182867            7.485681\n",
    "2010s          42.581666     32.045745       17.777845            7.397044\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, some intense plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploded pie chart\n",
    ">>> plotter('Pie chart of common modals in the NYT', each_md.results, explode = ['other'],\n",
    "...    num_to_plot = 'all', kind = 'pie', colours = 'Accent', figsize = (11, 11))\n",
    "\n",
    "# bar chart, transposing and reversing the data\n",
    ">>> plotter('Modals use by decade', modals.results.iloc[::-1].T.iloc[::-1], kind = 'barh',\n",
    "...    x_label = 'Percentage of all modals', y_label = 'Modal group')\n",
    "\n",
    "# stacked area chart\n",
    ">>> plotter('An ocean of modals', rel_modals.results.drop('1963'), kind = 'area', \n",
    "...    stacked = True, colours = 'summer', figsize = (8, 10), num_to_plot = 'all', \n",
    "...    legend_pos = 'lower right', y_label = 'Percentage of all modals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/pie-chart-of-common-modals-in-the-nyt2.png\"  height=\"400\" width=\"400\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/modals-use-by-decade.png\"  height=\"230\" width=\"500\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/interrogator/risk/master/images/an-ocean-of-modals2.png\"  height=\"600\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "<a name=\"more-information\"></a>\n",
    "## More information\n",
    "\n",
    "Some things are likely lacking documentation right now. For now, the more complex functionality of the toolkit is presented best in some of the research projects I'm working on:\n",
    "\n",
    "1. [Longitudinal linguistic change in an online support group](https://github.com/interrogator/sfl_corpling) (thesis project)\n",
    "2. [Discourse-semantics of *risk* in the NYT, 1963&ndash;2014](https://github.com/interrogator/risk) (most `corpkit` use)\n",
    "3. [Learning Python, IPython and NLTK by investigating a corpus of Malcolm Fraser's speeches](https://github.com/resbaz/nltk)\n",
    "\n",
    "<a name=\"cite\"></a>\n",
    "## Cite\n",
    "\n",
    "If you want to cite `corpkit`, please use:\n",
    "\n",
    "> `McDonald, D. (2015). corpkit: a toolkit for corpus linguistics. Retrieved from https://www.github.com/interrogator/corpkit. DOI: http://doi.org/10.5281/zenodo.28361`"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
